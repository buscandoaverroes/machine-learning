<h1><a href="#a-tutorialwalkthrough-of-basic-lasso-and-ridge-model-selection-techniques" id="a-tutorialwalkthrough-of-basic-lasso-and-ridge-model-selection-techniques">A tutorial/walkthrough of basic lasso and ridge model selection techniques</a></h1>
<p>This is a stylized version of a lasso and ridge commands in Stata. The best way to follow along is to download the accompanying &ldquo;lasso+ridge.do&rdquo; script which includes on the Stata commands from this file. Be sure to change the file paths to match those of your computer.</p>
<h2><a href="#introduction" id="introduction">Introduction</a></h2>
<p>From what little I know about machine learning (ML) is that Stata has been somewhat slow to the game. Version 16, released less than a year ago in the summer of 2019, finally included an in-house command <strong>lasso</strong> that performs many of the functions that were previously found only in community-written packages. I learned using one of these packages called lassopack &ndash; which includes the command <strong>lasso2</strong>. This packages is available on SSC and we&rsquo;ll use that package here to demonstrate lasso and ridge algorithms. I don&rsquo;t have much experience with the in-house command built into version 16 and the few times I have used it, I get results that I don&rsquo;t expect. When I have time I&rsquo;ll include a separate script that directly compares the results from <strong>lasso2</strong> and <strong>lasso</strong>. In short, I&rsquo;m skeptical of <strong>lasso</strong> because the command doesn&rsquo;t produce results that I know I should be getting, but I&rsquo;m very very open to the possibility that I&rsquo;m not doing something right. If that&rsquo;s the case, please comment and/or show me how to actually use <strong>lasso</strong> properly.</p>
<h2><a href="#getting-started" id="getting-started">Getting Started</a></h2>
<p>Anyway, The first thing we need to do is download lassopack from the SSC. We could do so by running the install command from the ssc, which would look like this</p>
<pre><code class="language-{stata,">. ssc install pdslasso

</code></pre>
<p>However, the authors of this particular package (see <a href="https://statalasso.github.io/">https://statalasso.github.io/</a>) note that they update their GitHub-hosted version more frequently than that on the SSC. We&rsquo;ll install directly from GitHub instead and use</p>
<pre><code class="language-{stata,">. net install lassopack, from(&quot;https://raw.githubusercontent.com/statalasso/lass
&gt; opack/master/lassopack_v131/&quot;) replace

</code></pre>
<p>We&rsquo;ll also be using the estout package to store results from the lasso commands. You probably know about estout but, if not, I highly recommend familiarizing yourself with it as it will become more and more useful to you over time.</p>
<pre><code class="language-{stata,">. ssc install estout, replace

</code></pre>
<p>Whereas traditional econometrics cares about making sense of relationships between people, actions, and things in the world, the machine-learning approach doesn&rsquo;t really care about any of that. Instead, machine-learning classifies, predicts, and seeks to build the best model independent of theoretical or historical context. Take Google; they just want to know &ldquo;yes&rdquo; or &ldquo;no&rdquo;. Will you click on the ad or not? They&rsquo;ve likely got hundreds of variables on &ldquo;you&rdquo; and they&rsquo;re pretty good at predicting the answer to that question, for each of us. The World Bank or J-PAL, residing comfortably in the classical econometrical tradition, care much more about making sure the model makes sense in context: does climate change incite violence? Interestingly, from an econometrics perspective, this question becomes very difficult to answer in the affirmative (even though we all know the meteorological and biological evidence of climate change is immense) since very few researchers have told a convincing story that isolates climate change-induced weather variability from other economic or political factors which also affect intermediate factors &ndash; such as food or water insecurity &ndash; that may incite violence. (A fantastic paper by Selby et al that examines climate change and the Syrian Revolution is linked here: <a href="https://www.sciencedirect.com/science/article/pii/S0962629816301822">https://www.sciencedirect.com/science/article/pii/S0962629816301822</a>) Econometrics cares about &ldquo;why&rdquo; and machine-learning cares about what the best model looks like; in a way the former is much harder to do well, in my opinion. But it&rsquo;s also pretty easy to have machine-learning go horribly wrong. The rest of this tutorial will hopefully set us up with some basic conceptual and coding understanding to, at least, not totally mess up machine-learning.</p>
<h2><a href="#traditional-approach" id="traditional-approach">Traditional Approach</a></h2>
<p>Now we need a dataset. Let&rsquo;s first use one of Stata&rsquo;s built-in datasets to demonstrate an instance where ML won&rsquo;t be as useful.</p>
<pre><code class="language-{stata,">. sysuse auto, clear

</code></pre>
<p>Now let&rsquo;s say we want to predict the car&rsquo;s price. An econometric approach might ask what factors predict price (in miles/gallon) by examining the context of the US auto market in the late 1970s. We might surmise that foreign cars, subject to import tariffs, are likely to be more expensive; also, cars that weigh more might indicate less efficiency. We could run lasso or ridge here, but using our rough &ldquo;theory of change&rdquo; we can tell a pretty convincing story already: foreign and weight are two strong predictors using OLS.</p>
<pre><code class="language-{stata}">. reg price       foreign weight

      Source |       SS           df       MS      Number of obs   =        74
-------------+----------------------------------   F(2, 71)        =     35.35
       Model |   316859273         2   158429637   Prob &gt; F        =    0.0000
    Residual |   318206123        71  4481776.38   R-squared       =    0.4989
-------------+----------------------------------   Adj R-squared   =    0.4848
       Total |   635065396        73  8699525.97   Root MSE        =      2117

------------------------------------------------------------------------------
       price |      Coef.   Std. Err.      t    P&gt;|t|     [95% Conf. Interval]
-------------+----------------------------------------------------------------
     foreign |   3637.001    668.583     5.44   0.000     2303.885    4970.118
      weight |   3.320737   .3958784     8.39   0.000     2.531378    4.110096
       _cons |  -4942.844   1345.591    -3.67   0.000    -7625.876   -2259.812
------------------------------------------------------------------------------

</code></pre>
<h2><a href="#lasso" id="lasso">Lasso</a></h2>
<p>Great, but what happens if we have a dataset about which we know very little? Let&rsquo;s take a look at this one about wine quality from the UCI machine learning repository. When you find a dataset you should always keep in mind what the owners allow you to do with it &ndash; here we&rsquo;re ok to use it for research purposes as long as we cite it, so we will: P. Cortez, A. Cerdeira, F. Almeida, T. Matos and J. Reis. Modeling wine preferences by data mining from physicochemical properties. In Decision Support Systems, Elsevier, 47(4):547-553, 2009.</p>
<pre><code class="language-{stata,">. import delimited using &quot;http://archive.ics.uci.edu/ml/machine-learning-databas
&gt; es/wine-quality/winequality-red.csv&quot; , clear

</code></pre>
<p>Great &ndash; now let&rsquo;s describe dataset:</p>
<pre><code class="language-{stata}">. describe, short

Contains data
  obs:         1,599                          
 vars:            12                          
Sorted by: 
     Note: Dataset has changed since last saved.

</code></pre>
<p>Looks like each observation is a wine sample, and we have 12 variables, including <strong>quality</strong> that tells us how &ldquo;good&rdquo; the wine is. Let&rsquo;s see if we can predict quality from the other variables at our disposal. I know nothing about the chemistry of wine, so I can&rsquo;t use my economic background to help me produce any sort of story. We&rsquo;ll use lasso to help us with <strong>model selection</strong>: it will tell us which variables to include minimize the sum of the error term.</p>
<p>We&rsquo;ll use the command called <strong>lasso2</strong> that we downloaded above. We&rsquo;ll focus on model selection here, but I recommend you check out the package website for all that it can do: <a href="https://statalasso.github.io/docs/lassopack/">https://statalasso.github.io/docs/lassopack/</a>. To see the documentation, type</p>
<pre><code class="language-{stata,">. help lasso2

</code></pre>
<p>If you look at the help file, you&rsquo;ll see that we need to tell Stata in order to make the command run: 1) a dependent variable, 2) and a list of candidate explanatory variables from which the algorithm will select the &ldquo;best fit&rdquo; model. If we browse, we see that <strong>quality</strong> is the main outcome variable. Let&rsquo;s create a global called <strong>winevars</strong> to store all of the potential explanatory variables.</p>
<pre><code class="language-{stata,">. global  winevars        fixedacidity volatileacidity ///
&gt;                                         citricacid residualsugar

.                                         chlorides freesulfurdioxide ///
&gt;                                         totalsulfurdioxide ph sulphates alcoho
&gt; l
command chlorides is unrecognized

</code></pre>
<p>Now let&rsquo;s run our lasso command. Remember that lasso is an algorithm that selects explanatory variables as a function of lamba, a coefficient &ldquo;penalty&rdquo;. The one option worth noting with this command is: alpha(1). This tells Stata to run a <strong>lasso</strong> algorithm as opposed to ridge (see more explanation in the theory file). I&rsquo;ll include other options that you can see explained in the do file.</p>
<pre><code class="language-{stata}">. lasso2 quality ${winevars} ///
&gt;                                 , plotpath(lambda) plotlabel ///
&gt;                                 plotvar(${winevars}) ///
&gt;                                 plotopt(legend(on)) alpha(1)

  Knot|  ID     Lambda    s      L1-Norm        EBIC     R-sq   | Action
------+---------------------------------------------------------+---------------
     1|   1 1008.34183     1     0.00000   -677.11968   0.0000  | Added _cons.
     2|   2  918.76352     2     0.15648   -711.69883   0.0259  | Added
      |                                                         | volatileacidit
&gt; y.
     3|  28   81.78971     3     1.61823   -925.16078   0.1516  | Added
      |                                                         | fixedacidity.
     4|  39   29.39370     4     1.69937   -920.45671   0.1530  | Added
      |                                                         | residualsugar.
     5|  59    4.57271     5     1.74881   -913.67292   0.1533  | Added
      |                                                         | citricacid.
Use long option for full output.
Type e.g. lasso2, lic(ebic) to run the model selected by EBIC.

</code></pre>
<p><img src="Graph12.svg" ></p>
<p>If we take a look at the full table, we notice that as lambda decreases, more variables are added to our model. (This is because the &ldquo;pentalty&rdquo; or adding additional variables decreases as lambda decreases.) We can see this trend by looking at the generated output graph. As lambda increases, the coefficents for our covariates decreases until they reach 0 &ndash; that is, when they are dropped from the model all together. But notice how some variables, such as <strong>citricacid</strong> &ldquo;drop off&rdquo; quicker than others, such as <strong>volatileacidity</strong>. One might interpret this as an indication that variables that are &ldquo;slower&rdquo; to drop off are better predictors of wine quality; this may be a general rule of thumb but is not always true. The iteration higlighted with a <strong>&ldquo;*&rdquo;</strong> will indicate the model with the lowest error (as measured by ebic). 7 variables are included in this model. Let&rsquo;s note the value of lambda that gave us our lowest &ldquo;error&rdquo;: 51.99287. This is our &ldquo;selected lambda&rdquo;</p>
<p>Our next command will run standard OLS with the 7 variable selected by the lasso algorithm. It won&rsquo;t give us the full output we&rsquo;re used to with <strong>reg</strong>, but it will at least give us the value of the coefficients:</p>
<pre><code class="language-{stata}">. lasso2, lic(ebic)

Use lambda=89.76411116338411 (selected by EBIC).

---------------------------------------------------
         Selected |           Lasso   Post-est OLS
------------------+--------------------------------
  volatileacidity |      -1.6046319     -1.7614378
------------------+--------------------------------
   Partialled-out*|
------------------+--------------------------------
            _cons |       6.4829802      6.5657455
---------------------------------------------------

</code></pre>
<h2><a href="#cross-validation" id="cross-validation">Cross-Validation</a></h2>
<p>Cross-validation is a means of testing out-of-sample fit. We want to develop a model that is valid not only for our data in the data, but, in our case, for all wine. We&rsquo;ll do this by dividing the dataset into <em>k</em> folds, or a certain number of equal divisions. The standard number is 10, so since our dataset has about 1,600 observations, each <em>fold</em> will have about 160 wine samples. The algorithm then takes a large majority of the folds and uses this subset to develop its lasso model as we did above. Then it uses the remaning folds to &ldquo;test&rdquo; the model. The assumption is that, with large enough datasets, a randomly-divided dataset will generate enough variance between the &ldquo;training&rdquo; and &ldquo;test&rdquo; divisions that the &ldquo;test&rdquo; portion can approximate the differences of out-of-sample data.</p>
<h3><a href="#a-side-note-on-training-and-test-data" id="a-side-note-on-training-and-test-data">A side-note on training and test data</a></h3>
<p>This assumption I just explained about gets at what I see as a fundamental problem of applying machine-learning techniques to development problems: heterogeneity in data availability. Let&rsquo;s use an invented example. Let&rsquo;s say I want to predict likelihood of forest fires based on satellite images. I collect my data using the best publicly available images from NASA or NOAA that I can find, develop my theory of change, and maybe use lasso to fine-tune my model. This is great, but suppose NASA has much better quality images of North America than anywhere else. Even if I train my model on images from around the world, can I really say that my &ldquo;test&rdquo; data is good enough to create valid model for terrain that has different tree species, different roof coverings, and other differences that aren&rsquo;t captured in the lesser-quality images?</p>
<h2><a href="#back-to-cross-validation" id="back-to-cross-validation">Back to Cross-validation</a></h2>
<p>Lassopack also includes a cross-validation command called <strong>cvlasso</strong> that works very similarly to <strong>lasso2</strong>. We&rsquo;ll run this command, and also capture the selected varialbes in a global so we can run a better OLS command than last time. Keep in mind that if you type</p>
<pre><code class="language-{stata,">. help cvlasso

</code></pre>
<p>you&rsquo;ll see that you can ajust the number of k-folds in the options.</p>
<pre><code class="language-{stata}">. cvlasso         quality ${winevars} ///
&gt;                         , plotcv seed(123) lopt alpha(1) postest

K-fold cross-validation with 10 folds. Elastic net with alpha=1.
Fold 1 2 3 4 5 6 7 8 9 10 
          |         Lambda           MSPE       st. dev.
----------+---------------------------------------------
         1|      1008.3418      .65131479       .0223197  
         2|      918.76352      .63627149      .02174724  
         3|      837.14311      .62217786      .02076667  
         4|      762.77362      .61047734      .01994615  
         5|      695.01091      .60076361      .01925827  
         6|      633.26806      .59269932      .01868041  
         7|      577.01027       .5860044      .01819395  
         8|      525.75027      .58044634      .01778355  
         9|      479.04406      .57583211      .01743655  
        10|      436.48711      .57200145      .01714248  
        11|       397.7108      .56882131      .01689271  ^
        12|      362.37927      .56618122      .01668009  
        13|      330.18649      .56398949      .01649866  
        14|      300.85363      .56216998      .01634351  
        15|      274.12663      .56065948      .01621052  
        16|      249.77397      .55940553      .01609626  
        17|      227.58474      .55836456      .01599787  
        18|      207.36674      .55750039      .01591295  
        19|      188.94485      .55678302      .01583949  
        20|      172.15951       .5561875       .0157758  
        21|      156.86533      .55569314      .01572045  
        22|      142.92985      .55528277      .01567224  
        23|      130.23236       .5549518      .01562854  
        24|      118.66287      .55472942      .01557949  
        25|      108.12119      .55456506      .01553216  
        26|      98.516001      .55444559       .0154905  
        27|      89.764111      .55437042      .01545617  
        28|      81.789715      .55430874      .01542484  
        29|      74.523742      .55426189      .01539515  
        30|      67.903258      .55423364      .01536928  
        31|       61.87092      .55421047      .01535176  
        32|      56.374478      .55419819      .01534066  *
        33|      51.366325      .55420873      .01533232  
        34|      46.803082      .55423424      .01532959  
        35|      42.645225      .55426779      .01532958  
        36|      38.856741       .5543072      .01532826  
        37|      35.404815      .55434925       .0153272  
        38|      32.259549      .55439109      .01532627  
        39|        29.3937      .55443185      .01532555  
        40|      26.782444      .55447149      .01532519  
        41|      24.403166      .55450642      .01532603  
        42|      22.235256      .55454039      .01532689  
        43|      20.259937      .55457298      .01532778  
        44|        18.4601      .55462753      .01534374  
        45|      16.820156      .55469167      .01536667  
        46|      15.325899      .55475153      .01538795  
        47|      13.964389      .55479173      .01541062  
        48|      12.723831      .55482881      .01542978  
        49|      11.593481       .5548688      .01544855  
        50|      10.563548       .5549268      .01547055  
        51|      9.6251114      .55498171       .0154907  
        52|       8.770043      .55503821      .01550849  
        53|      7.9909366       .5550937      .01552424  
        54|      7.2810438      .55514735      .01553824  
        55|      6.6342159      .55519693      .01555107  
        56|      6.0448505      .55524269      .01556283  
        57|      5.5078426      .55528486      .01557359  
        58|      5.0185411      .55532368      .01558345  
        59|      4.5727077      .55535938      .01559247  
        60|       4.166481      .55539215      .01560071  
        61|      3.7963423      .55541975      .01560787  
        62|      3.4590857      .55544561      .01561451  
        63|      3.1517901      .55546934      .01562057  
        64|      2.8717937      .55549109      .01562611  
        65|      2.6166715      .55551102      .01563117  
        66|      2.3842136      .55552927      .01563579  
        67|      2.1724066      .55554597      .01564001  
        68|       1.979416      .55556126      .01564387  
        69|      1.8035701      .55557523      .01564739  
        70|      1.6433459      .55558801       .0156506  
        71|      1.4973556      .55559969      .01565353  
        72|      1.3643347      .55561037       .0156562  
        73|      1.2431309      .55562011      .01565864  
        74|      1.1326946      .55562902      .01566087  
        75|      1.0320691      .55563715       .0156629  
        76|      .94038297      .55564457      .01566475  
        77|      .85684194      .55565134      .01566643  
        78|      .78072246      .55565752      .01566797  
        79|      .71136523      .55566267      .01566922  
        80|       .6481695      .55566738      .01567037  
        81|      .59058791      .55567116      .01567174  
        82|      .53812171      .55567459      .01567301  
        83|      .49031646      .55567772      .01567417  
        84|       .4467581      .55568057      .01567522  
        85|      .40706934       .5556794      .01567693  
        86|      .37090642       .5556814      .01567788  
        87|      .33795612      .55568324      .01567874  
        88|      .30793303      .55568491      .01567953  
        89|       .2805771      .55568643      .01568025  
        90|      .25565141      .55568782       .0156809  
        91|      .23294004      .55568909      .01568149  
        92|      .21224629      .55569025      .01568204  
        93|      .19339092       .5556913      .01568253  
        94|       .1762106      .55569226      .01568298  
        95|      .16055654      .55569314      .01568339  
        96|      .14629314      .55569394      .01568377  
        97|      .13329686      .55569466      .01568411  
        98|      .12145513      .55569533      .01568442  
        99|      .11066539      .55569593       .0156847  
       100|      .10083418      .55569648      .01568496  
* lopt = the lambda that minimizes MSPE.
  Run model: cvlasso, lopt
^ lse = largest lambda for which MSPE is within one standard error of the
        minimal MSPE.
  Run model: cvlasso, lse
Estimate lasso with lambda=56.374 (lopt).

---------------------------------------------------
         Selected |           Lasso   Post-est OLS
------------------+--------------------------------
     fixedacidity |       0.0038595      0.0119222
  volatileacidity |      -1.6533470     -1.7317454
------------------+--------------------------------
   Partialled-out*|
------------------+--------------------------------
            _cons |       6.4765833      6.4508846
---------------------------------------------------

. global          lassovars = e(selected)

</code></pre>
<p><img src="Graph13.svg" > Notice how the graph now is different: we see the natural log of lambda (indicated by a red line) that generated the model with the lowest error. Notice that the corss-validation method gave us a different ideal lambda value. Since we stored the variables the cvlasso command selected, we can run OLS with these variables with</p>
<pre><code class="language-{stata}">. reg             quality         ${lassovars}

      Source |       SS           df       MS      Number of obs   =     1,599
-------------+----------------------------------   F(2, 1596)      =    144.32
       Model |  159.610433         2  79.8052164   Prob &gt; F        =    0.0000
    Residual |   882.55467     1,596  .552979117   R-squared       =    0.1532
-------------+----------------------------------   Adj R-squared   =    0.1521
       Total |   1042.1651     1,598    .6521684   Root MSE        =    .74363

-------------------------------------------------------------------------------
      quality |      Coef.   Std. Err.      t    P&gt;|t|     [95% Conf. Interval]
--------------+----------------------------------------------------------------
 fixedacidity |   .0119222   .0110529     1.08   0.281    -.0097576     .033602
volatileaci~y |  -1.731745   .1074738   -16.11   0.000     -1.94255   -1.520941
        _cons |   6.450885   .1212101    53.22   0.000     6.213137    6.688632
-------------------------------------------------------------------------------

</code></pre>
<p>Now that we have significance values, we can look and see which chemical components are strong predictors of quality.</p>
<h2><a href="#ridge" id="ridge">Ridge</a></h2>
<p>Recall that ridge operates very similarly to lasso. The main differences is that, due to squared constraint term, no coveriates will every be <em>completely</em> dropped from the model; &ldquo;unimportant&rdquo; covariates will only see their coefficents reduced to near-zero.</p>
<p>Let&rsquo;s use a new dataset to explore ridge. This one from Prof. Hastie for predicting rates of prostate cancer. (Btw, his website is here for more info and datasets: <a href="http://web.stanford.edu/~hastie/pub.htm">http://web.stanford.edu/~hastie/pub.htm</a>)</p>
<pre><code class="language-{stata,">. import delimited using &quot;https://web.stanford.edu/~hastie/ElemStatLearn/dataset
&gt; s/prostate.data&quot;, clear

</code></pre>
<p>We&rsquo;ll follow the steps above, except we&rsquo;ll tell stata to set <em>alpha=0</em> in the options, which indicates a ridge regression. <strong>Lpsa</strong> is our outcome variable of interest, and we&rsquo;ll group the explanatory variables in a global like last time. Let&rsquo;s run a lasso before ridge so we can compare.</p>
<pre><code class="language-{stata}">. global          rhsvars lcavol lweight ///
&gt;                                         age lbph svi lcp gleason pgg45

. lasso2          lpsa ${rhsvars} , plotpath(lambda) plotlabel ///
&gt;                                 plotvar(${rhsvars}) ///
&gt;                                 plotopt(legend(on)) alpha(1) ///
&gt;                                 lic(ebic) postresults long

  Knot|  ID     Lambda    s      L1-Norm        EBIC     R-sq   | Action
------+---------------------------------------------------------+---------------
     1|   1  163.62492     1     0.00000     31.41226   0.0000  | Added _cons.
     2|   2  149.08894     2     0.06390     26.66962   0.0916  | Added lcavol.
      |   3  135.84429     2     0.12213     18.19047   0.1676  |
      |   4  123.77625     2     0.17518     10.54017   0.2307  |
      |   5  112.78031     2     0.22352      3.69568   0.2832  |
      |   6  102.76122     2     0.26757     -2.37828   0.3267  |
      |   7   93.63220     2     0.30770     -7.72701   0.3628  |
      |   8   85.31417     2     0.34427    -12.40327   0.3928  |
     3|   9   77.73509     3     0.40800    -12.63533   0.4221  | Added svi.
      |  10   70.82932     3     0.48389    -17.25324   0.4490  |
     4|  11   64.53704     4     0.60174    -18.31145   0.4801  | Added
      |                                                         | lweight.
      |  12   58.80375     4     0.71293    -23.37859   0.5066  |
      |  13   53.57979     4     0.81423    -27.79633   0.5285  |
      |  14   48.81991     4     0.90654    -31.62332   0.5468  |
      |  15   44.48288     4     0.99065    -34.91944   0.5619  |
      |  16   40.53114     4     1.06728    -37.74368   0.5745  |
      |  17   36.93047     4     1.13711    -40.15255   0.5849  |
      |  18   33.64967     4     1.20073    -42.19891   0.5936  |
      |  19   30.66032     4     1.25870    -43.93126   0.6008  |
      |  20   27.93654     4     1.31152    -45.39336*  0.6067  |
     5|  21   25.45474     5     1.35340    -42.20238   0.6123  | Added pgg45.
     6|  22   23.19341     6     1.39138    -38.93672   0.6175  | Added lbph.
      |  23   21.13297     6     1.42643    -40.17769   0.6224  |
      |  24   19.25558     6     1.45836    -41.22016   0.6264  |
      |  25   17.54496     6     1.48746    -42.09423   0.6298  |
      |  26   15.98632     6     1.51397    -42.82594   0.6325  |
      |  27   14.56614     6     1.53812    -43.43763   0.6349  |
      |  28   13.27212     6     1.56013    -43.94842   0.6368  |
     7|  29   12.09306     7     1.58269    -39.94418   0.6389  | Added age.
      |  30   11.01875     7     1.61022    -40.80902   0.6421  |
      |  31   10.03987     7     1.63531    -41.53293   0.6448  |
      |  32    9.14796     7     1.65817    -42.13807   0.6470  |
      |  33    8.33528     7     1.67900    -42.64335   0.6488  |
      |  34    7.59480     7     1.69798    -43.06485   0.6503  |
     8|  35    6.92010     8     1.71689    -38.84649   0.6516  | Added
      |                                                         | gleason.
      |  36    6.30533     8     1.73692    -39.15171   0.6527  |
      |  37    5.74518     8     1.75517    -39.40584   0.6536  |
      |  38    5.23480     8     1.77180    -39.61733   0.6544  |
      |  39    4.76975     8     1.78695    -39.79327   0.6550  |
      |  40    4.34602     8     1.80076    -39.93957   0.6555  |
     9|  41    3.95993     9     1.83346    -35.69248   0.6567  | Added lcp.
      |  42    3.60814     9     1.86831    -36.01479   0.6578  |
      |  43    3.28761     9     1.90006    -36.28320   0.6588  |
      |  44    2.99554     9     1.92900    -36.50659   0.6596  |
      |  45    2.72943     9     1.95536    -36.69246   0.6602  |
      |  46    2.48695     9     1.97938    -36.84703   0.6607  |
      |  47    2.26602     9     2.00126    -36.97555   0.6612  |
      |  48    2.06471     9     2.02120    -37.08238   0.6616  |
      |  49    1.88129     9     2.03937    -37.17116   0.6619  |
      |  50    1.71416     9     2.05593    -37.24493   0.6621  |
      |  51    1.56188     9     2.07101    -37.30622   0.6623  |
      |  52    1.42313     9     2.08476    -37.35713   0.6625  |
      |  53    1.29670     9     2.09728    -37.39942   0.6627  |
      |  54    1.18150     9     2.10869    -37.43454   0.6628  |
      |  55    1.07654     9     2.11909    -37.46371   0.6629  |
      |  56    0.98091     9     2.12856    -37.48793   0.6630  |
      |  57    0.89376     9     2.13720    -37.50804   0.6630  |
      |  58    0.81437     9     2.14506    -37.52475   0.6631  |
      |  59    0.74202     9     2.15223    -37.53862   0.6632  |
      |  60    0.67610     9     2.15876    -37.55013   0.6632  |
      |  61    0.61604     9     2.16471    -37.55969   0.6632  |
      |  62    0.56131     9     2.17013    -37.56763   0.6633  |
      |  63    0.51145     9     2.17507    -37.57422   0.6633  |
      |  64    0.46601     9     2.17957    -37.57970   0.6633  |
      |  65    0.42461     9     2.18367    -37.58424   0.6633  |
      |  66    0.38689     9     2.18741    -37.58801   0.6633  |
      |  67    0.35252     9     2.19081    -37.59115   0.6633  |
      |  68    0.32120     9     2.19391    -37.59375   0.6633  |
      |  69    0.29267     9     2.19674    -37.59590   0.6634  |
      |  70    0.26667     9     2.19932    -37.59770   0.6634  |
      |  71    0.24298     9     2.20166    -37.59919   0.6634  |
      |  72    0.22139     9     2.20380    -37.60042   0.6634  |
      |  73    0.20172     9     2.20575    -37.60145   0.6634  |
      |  74    0.18380     9     2.20752    -37.60230   0.6634  |
      |  75    0.16748     9     2.20914    -37.60301   0.6634  |
      |  76    0.15260     9     2.21062    -37.60359   0.6634  |
      |  77    0.13904     9     2.21196    -37.60408   0.6634  |
      |  78    0.12669     9     2.21318    -37.60448   0.6634  |
      |  79    0.11543     9     2.21430    -37.60482   0.6634  |
      |  80    0.10518     9     2.21531    -37.60510   0.6634  |
      |  81    0.09584     9     2.21624    -37.60533   0.6634  |
      |  82    0.08732     9     2.21708    -37.60552   0.6634  |
      |  83    0.07956     9     2.21785    -37.60568   0.6634  |
      |  84    0.07250     9     2.21855    -37.60581   0.6634  |
      |  85    0.06606     9     2.21919    -37.60592   0.6634  |
      |  86    0.06019     9     2.21977    -37.60602   0.6634  |
      |  87    0.05484     9     2.22030    -37.60609   0.6634  |
      |  88    0.04997     9     2.22078    -37.60615   0.6634  |
      |  89    0.04553     9     2.22122    -37.60621   0.6634  |
      |  90    0.04148     9     2.22162    -37.60625   0.6634  |
      |  91    0.03780     9     2.22199    -37.60629   0.6634  |
      |  92    0.03444     9     2.22232    -37.60632   0.6634  |
      |  93    0.03138     9     2.22262    -37.60634   0.6634  |
      |  94    0.02859     9     2.22290    -37.60636   0.6634  |
      |  95    0.02605     9     2.22315    -37.60638   0.6634  |
      |  96    0.02374     9     2.22338    -37.60639   0.6634  |
      |  97    0.02163     9     2.22359    -37.60640   0.6634  |
      |  98    0.01971     9     2.22378    -37.60641   0.6634  |
      |  99    0.01796     9     2.22395    -37.60642   0.6634  |
      | 100    0.01636     9     2.22411    -37.60643   0.6634  |
*indicates minimum EBIC.

Use lambda=27.93654463358689 (selected by EBIC).

---------------------------------------------------
         Selected |           Lasso   Post-est OLS
------------------+--------------------------------
           lcavol |       0.4725389      0.5258519
          lweight |       0.3989102      0.6617699
              svi |       0.4400748      0.6656665
------------------+--------------------------------
   Partialled-out*|
------------------+--------------------------------
            _cons |       0.2975585     -0.7771568
---------------------------------------------------
Plotting only supported for list of lambda values.
Plotting options ignored.

. global                          lpsalasso  = e(selected)

. graph export            &quot;output/lpsa-lasso-graph.png&quot;, replace
(file output/lpsa-lasso-graph.png written in PNG format)

</code></pre>
<p><img src="Graph14.svg" > Note our selected lambda value, and that we have three covariates with that value. Now let&rsquo;s run the same selection of variables with ridge.</p>
<pre><code class="language-{stata}">. lasso2          lpsa ${rhsvars} , plotpath(lambda) plotlabel ///
&gt;                                         plotvar(${rhsvars}) ///
&gt;                                         plotopt(legend(on)) alpha(0) ///
&gt;                                         lic(ebic) postresults long

  Knot|  ID     Lambda    s      L1-Norm        EBIC     R-sq   | Action
------+---------------------------------------------------------+---------------
     1|   1  1.636e+05     7     0.00550     31.10766   0.0036  | Added lcavol
      |                                                         | lweight lbph
      |                                                         | svi lcp
      |                                                         | gleason _cons.
      |   2  1.491e+05     7     0.00603     31.07807   0.0039  |
      |   3  1.358e+05     7     0.00661     31.04562   0.0043  |
      |   4  1.238e+05     7     0.00726     31.01004   0.0047  |
      |   5  1.128e+05     7     0.00796     30.97101   0.0052  |
      |   6  1.028e+05     7     0.00873     30.92823   0.0057  |
      |   7  9.363e+04     7     0.00958     30.88132   0.0062  |
      |   8  8.531e+04     7     0.01051     30.82989   0.0068  |
      |   9  7.774e+04     7     0.01152     30.77351   0.0075  |
      |  10  7.083e+04     7     0.01264     30.71173   0.0082  |
      |  11  6.454e+04     7     0.01386     30.64401   0.0090  |
      |  12  5.880e+04     7     0.01520     30.56982   0.0099  |
      |  13  5.358e+04     7     0.01666     30.48854   0.0108  |
     2|  14  4.882e+04     8     0.01837     30.37788   0.0121  | Added age.
      |  15  4.448e+04     8     0.02014     30.27832   0.0132  |
      |  16  4.053e+04     8     0.02207     30.16931   0.0145  |
      |  17  3.693e+04     8     0.02419     30.04999   0.0159  |
      |  18  3.365e+04     8     0.02651     29.91940   0.0174  |
     3|  19  3.066e+04     9     0.02916     29.56364   0.0212  | Added pgg45.
      |  20  2.794e+04     9     0.03194     29.38726   0.0232  |
      |  21  2.545e+04     9     0.03498     29.19446   0.0254  |
      |  22  2.319e+04     9     0.03831     28.98378   0.0277  |
      |  23  2.113e+04     9     0.04195     28.75367   0.0303  |
      |  24  1.926e+04     9     0.04592     28.50245   0.0332  |
      |  25  1.754e+04     9     0.05025     28.22832   0.0362  |
      |  26  1.599e+04     9     0.05498     27.92935   0.0396  |
      |  27  1.457e+04     9     0.06014     27.60349   0.0432  |
      |  28  1.327e+04     9     0.06575     27.24856   0.0471  |
      |  29  1.209e+04     9     0.07187     26.86224   0.0514  |
      |  30  1.102e+04     9     0.07853     26.44207   0.0560  |
      |  31  1.004e+04     9     0.08577     25.98547   0.0610  |
      |  32 9147.95888     9     0.09364     25.48976   0.0664  |
      |  33 8335.27943     9     0.10217     24.95213   0.0723  |
      |  34 7594.79618     9     0.11143     24.36965   0.0785  |
      |  35 6920.09542     9     0.12146     23.73936   0.0853  |
      |  36 6305.33321     9     0.13232     23.05820   0.0925  |
      |  37 5745.18479     9     0.14405     22.32310   0.1003  |
      |  38 5234.79841     9     0.15671     21.53101   0.1086  |
      |  39 4769.75334     9     0.17035     20.67891   0.1174  |
      |  40 4346.02160     9     0.18503     19.76390   0.1268  |
      |  41 3959.93302     9     0.20080     18.78321   0.1368  |
      |  42 3608.14349     9     0.21771     17.73433   0.1473  |
      |  43 3287.60596     9     0.23581     16.61501   0.1585  |
      |  44 2995.54411     9     0.25515     15.42341   0.1702  |
      |  45 2729.42823     9     0.27576     14.15813   0.1825  |
      |  46 2486.95335     9     0.29768     12.81831   0.1954  |
      |  47 2266.01927     9     0.32095     11.40377   0.2087  |
      |  48 2064.71236     9     0.34557      9.91503   0.2226  |
      |  49 1881.28900     9     0.37157      8.35342   0.2369  |
      |  50 1714.16047     9     0.39895      6.72119   0.2516  |
      |  51 1561.87918     9     0.42769      5.02152   0.2667  |
      |  52 1423.12614     9     0.45777      3.25861   0.2821  |
      |  53 1296.69954     9     0.48918      1.43767   0.2977  |
      |  54 1181.50432     9     0.52185     -0.43509   0.3135  |
      |  55 1076.54274     9     0.55575     -2.35244   0.3294  |
      |  56  980.90565     9     0.59079     -4.30628   0.3453  |
      |  57  893.76469     9     0.62690     -6.28767   0.3611  |
      |  58  814.36510     9     0.66398     -8.28701   0.3768  |
      |  59  742.01915     9     0.70195    -10.29418   0.3923  |
      |  60  676.10021     9     0.74068    -12.29875   0.4075  |
      |  61  616.03733     9     0.78006    -14.29014   0.4223  |
      |  62  561.31027     9     0.81997    -16.25788   0.4368  |
      |  63  511.44502     9     0.86029    -18.19177   0.4509  |
      |  64  466.00965     9     0.90088    -20.08212   0.4644  |
      |  65  424.61064     9     0.94161    -21.91988   0.4775  |
      |  66  386.88940     9     0.98236    -23.69681   0.4900  |
      |  67  352.51921     9     1.02300    -25.40561   0.5020  |
      |  68  321.20238     9     1.06340    -27.03996   0.5134  |
      |  69  292.66764     9     1.10345    -28.59459   0.5242  |
     4|  70  266.66786     8     1.14310    -30.06520   0.5345  | Removed age.
     5|  71  242.97782     9     1.18294    -31.44891   0.5442  | Added age.
      |  72  221.39234     9     1.22213    -32.74329   0.5533  |
      |  73  201.72445     9     1.26063    -33.94720   0.5619  |
      |  74  183.80381     9     1.29835    -35.06032   0.5700  |
      |  75  167.47519     9     1.33521    -36.08310   0.5776  |
      |  76  152.59715     9     1.37114    -37.01671   0.5847  |
      |  77  139.04084     9     1.40608    -37.86296   0.5914  |
      |  78  126.68884     9     1.43999    -38.62419   0.5976  |
      |  79  115.43415     9     1.47280    -39.30319   0.6033  |
      |  80  105.17930     9     1.50449    -39.90316   0.6087  |
      |  81   95.83546     9     1.53503    -40.42759   0.6137  |
      |  82   87.32170     9     1.56438    -40.88022   0.6183  |
      |  83   79.56428     9     1.59254    -41.26496   0.6226  |
      |  84   72.49601     9     1.61950    -41.58586   0.6266  |
      |  85   66.05566     9     1.64525    -41.84705   0.6302  |
      |  86   60.18746     9     1.66979    -42.05269   0.6336  |
      |  87   54.84057     9     1.69314    -42.20694   0.6366  |
      |  88   49.96869     9     1.71530    -42.31395   0.6395  |
      |  89   45.52961     9     1.73628    -42.37780   0.6421  |
      |  90   41.48488     9     1.75613    -42.40252*  0.6444  |
      |  91   37.79948     9     1.77484    -42.39203   0.6465  |
      |  92   34.44148     9     1.79247    -42.35016   0.6485  |
      |  93   31.38179     9     1.80902    -42.28063   0.6502  |
      |  94   28.59392     9     1.82770    -42.18699   0.6518  |
      |  95   26.05372     9     1.85317    -42.07267   0.6532  |
      |  96   23.73917     9     1.87753    -41.94094   0.6545  |
      |  97   21.63025     9     1.90079    -41.79487   0.6556  |
      |  98   19.70868     9     1.92293    -41.63735   0.6566  |
      |  99   17.95782     9     1.94395    -41.47105   0.6575  |
      | 100   16.36249     9     1.96387    -41.29846   0.6583  |
*indicates minimum EBIC.

Use lambda=41.48488213793183 (selected by EBIC).

---------------------------------------------------
         Selected |           Ridge   Post-est OLS
------------------+--------------------------------
           lcavol |       0.4092614      0.5643413
          lweight |       0.5623717      0.6220198
              age |      -0.0113673     -0.0212482
             lbph |       0.0731408      0.0967125
              svi |       0.6032902      0.7616733
              lcp |       0.0204348     -0.1060509
          gleason |       0.0734774      0.0492279
            pgg45 |       0.0027821      0.0044575
------------------+--------------------------------
   Partialled-out*|
------------------+--------------------------------
            _cons |      -0.0872316      0.1815609
---------------------------------------------------
Plotting only supported for list of lambda values.
Plotting options ignored.

. global                          lpsaridge  = e(selected)

. graph export            &quot;output/lpsa-ridge-graph.png&quot;, replace
(file output/lpsa-ridge-graph.png written in PNG format)

</code></pre>
<p><img src="Graph15.svg" > Notice how the ridge graph coefficent paths all approach zero as lambda approaches infinity, but never actually reach it. Likewise, if we compare the two following regressions with the lasso- and ridge-selected variables, we notice that the ridge regression includes all variables.</p>
<pre><code class="language-{stata}">. eststo lasso: reg lpsa ${lpsalasso}

      Source |       SS           df       MS      Number of obs   =        97
-------------+----------------------------------   F(3, 93)        =     54.15
       Model |  81.3492223         3  27.1164074   Prob &gt; F        =    0.0000
    Residual |  46.5684363        93  .500735874   R-squared       =    0.6359
-------------+----------------------------------   Adj R-squared   =    0.6242
       Total |  127.917659        96  1.33247561   Root MSE        =    .70763

------------------------------------------------------------------------------
        lpsa |      Coef.   Std. Err.      t    P&gt;|t|     [95% Conf. Interval]
-------------+----------------------------------------------------------------
      lcavol |   .5258519   .0748632     7.02   0.000     .3771884    .6745154
     lweight |   .6617699   .1756352     3.77   0.000     .3129933    1.010547
         svi |   .6656665   .2070898     3.21   0.002     .2544271    1.076906
       _cons |  -.7771568   .6229995    -1.25   0.215     -2.01431    .4599967
------------------------------------------------------------------------------

. eststo ridge: reg lpsa ${lpsaridge}

      Source |       SS           df       MS      Number of obs   =        97
-------------+----------------------------------   F(8, 88)        =     21.68
       Model |  84.8592398         8   10.607405   Prob &gt; F        =    0.0000
    Residual |  43.0584188        88  .489300213   R-squared       =    0.6634
-------------+----------------------------------   Adj R-squared   =    0.6328
       Total |  127.917659        96  1.33247561   Root MSE        =     .6995

------------------------------------------------------------------------------
        lpsa |      Coef.   Std. Err.      t    P&gt;|t|     [95% Conf. Interval]
-------------+----------------------------------------------------------------
      lcavol |   .5643413   .0878335     6.43   0.000     .3897908    .7388918
     lweight |   .6220198   .2008967     3.10   0.003     .2227799     1.02126
         age |  -.0212482   .0110841    -1.92   0.058    -.0432755    .0007791
        lbph |   .0967125   .0579127     1.67   0.098    -.0183768    .2118018
         svi |   .7616733   .2411757     3.16   0.002     .2823873    1.240959
         lcp |  -.1060509    .089868    -1.18   0.241    -.2846446    .0725427
     gleason |   .0492279   .1553407     0.32   0.752     -.259479    .3579349
       pgg45 |   .0044575   .0043653     1.02   0.310    -.0042177    .0131327
       _cons |   .1815609   1.320568     0.14   0.891    -2.442791    2.805913
------------------------------------------------------------------------------

</code></pre>
